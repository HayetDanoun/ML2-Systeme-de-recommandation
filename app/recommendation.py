# -*- coding: utf-8 -*-
"""recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19PE9UshfKDAXIxwve1Ebd6jcbf7iedbu
"""

# 1) Installer au besoin les librairies :
#!pip install kagglehub transformers datasets faiss-cpu

import os
import kagglehub
import pandas as pd
import torch

from datasets import Dataset
from transformers import (
    DPRContextEncoder,
    DPRContextEncoderTokenizer,
    RagTokenizer,
    RagTokenForGeneration,
    RagRetriever
)

# ====================================================
# ÉTAPE A : TÉLÉCHARGER ET CHARGER LE DATASET KAGGLE
# ====================================================
path = kagglehub.dataset_download("rounakbanik/the-movies-dataset")  # Téléchargement
csv_path = os.path.join(path, "movies_metadata.csv")                 # Fichier principal

df = pd.read_csv(csv_path, low_memory=False)

# Garder uniquement 'title' et 'overview' (renommé en 'text')
df = df.rename(columns={'overview': 'text'})
df = df[['title', 'text']].dropna()

print("Taille du DataFrame avant nettoyage :", len(df))

# Facultatif : On limite la taille brute des textes (ex. 5000 char max)
df['title'] = df['title'].astype(str)
df['text'] = df['text'].astype(str).apply(lambda x: x[:5000])

print("Taille du DataFrame après nettoyage :", len(df))

# ====================================================
# ÉTAPE B : CRÉER LE DATASET HUGGINGFACE
# ====================================================
dataset_hf = Dataset.from_pandas(df)  # Contient 'title', 'text', etc.

# ====================================================
# ÉTAPE C : CHARGER UN ENCODEUR DPR POUR LES EMBEDDINGS
# ====================================================
ctx_encoder_name = "facebook/dpr-ctx_encoder-single-nq-base"
ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(ctx_encoder_name)
ctx_encoder = DPRContextEncoder.from_pretrained(ctx_encoder_name)

device = "cpu"  # ou "cuda" si vous avez un GPU
ctx_encoder = ctx_encoder.to(device)
ctx_encoder.eval()

def embed_passages(examples):
    # Tronquer à max_length=512 pour éviter l'erreur 553 vs 512
    inputs = ctx_tokenizer(
        examples["text"],
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        output = ctx_encoder(**inputs)
    return {"embeddings": output.pooler_output.cpu().numpy()}

# ====================================================
# ÉTAPE D : CALCULER LES EMBEDDINGS ET AJOUTER L’INDEX FAISS
# ====================================================
# 1. map() => NOUVEAU dataset enrichi
dataset_hf = dataset_hf.map(embed_passages, batched=True, batch_size=8)

# 2. add_faiss_index => crée un index FAISS en mémoire
dataset_hf = dataset_hf.add_faiss_index(column="embeddings")

# ÉTAPE E : SAUVEGARDER L’INDEX FAISS, ENSUITE ENLEVER L’INDEX ET SAUVEGARDER LE DATASET
# ====================================================
import os

save_path = "my_custom_dataset"                     # Dossier pour le dataset
index_path = os.path.join(save_path, "embeddings")  # Dossier/fichier pour l'index

os.makedirs(save_path, exist_ok=True)               # Créer le dossier s'il n'existe pas

# 1. Vérifier et ajouter l'index FAISS
if not dataset_hf.is_index_initialized("embeddings"):
    dataset_hf.add_faiss_index(column="embeddings")

# 2. Sauvegarder l’index FAISS en mémoire dans un chemin dédié
dataset_hf.get_index("embeddings").save(index_path)

# 3. Supprimer l’index du dataset (modifie dataset_hf en place)
dataset_hf.drop_index("embeddings")

# 4. Sauvegarder le dataset sans l’index
dataset_hf.save_to_disk(save_path)

print(f"Dataset sans index sauvegardé dans '{save_path}'")
print(f"Index FAISS sauvegardé dans '{index_path}'")

import os
print(os.listdir("my_custom_dataset"))

# ====================================================
# ÉTAPE F : CRÉER UN RAG RETRIEVER À PARTIR DE L’INDEX ET DU DATASET
# ====================================================
model_name = "facebook/rag-token-base"
tokenizer = RagTokenizer.from_pretrained(model_name)
model = RagTokenForGeneration.from_pretrained(model_name)

retriever = RagRetriever.from_pretrained(
    model_name,
    index_name="custom",     # on n’utilise pas "wiki"
    passages_path=save_path, # chemin du dataset local
    index_path=index_path    # chemin de l’index FAISS
)

# retriever = RagRetriever.from_pretrained(
#     model_name,
#     index_name="custom",
#     passages_path="my_custom_dataset",
#     index_path="my_custom_dataset/embeddings"
# )

# !pip show transformers datasets

# ====================================================
# ÉTAPE G : FONCTION ET TEST DE RECOMMANDATION
# ====================================================
retriever = RagRetriever.from_pretrained(
    model_name,
    index_name="custom",
    passages_path="my_custom_dataset",
    index_path="my_custom_dataset/embeddings"
)

def recommend_with_rag(user_query, top_n=3):
    # 1. Tokeniser la question utilisateur
    inputs = tokenizer(user_query, return_tensors="pt")
    input_ids = inputs["input_ids"].to(device)

    # 2. Encoder la question pour obtenir les hidden states
    with torch.no_grad():
        question_hidden_states = model.question_encoder(input_ids=input_ids)[0]

    # 3. Convertir en NumPy pour FAISS
    question_hidden_states_numpy = question_hidden_states.cpu().numpy()

    # 4. Utiliser le retriever
    docs_dict = retriever(
        question_hidden_states=question_hidden_states_numpy,
        question_input_ids=input_ids,
        n_docs=top_n
    )

    # Debug : Vérifier les clés disponibles
    print("\n=== Structure de docs_dict ===")
    print(docs_dict.keys())

    # 5. Extraire les IDs des documents récupérés
    doc_ids = docs_dict["doc_ids"][0]  # Liste des indices FAISS retournés

    # 6. Récupérer les films correspondants dans dataset_hf
    retrieved_movies = dataset_hf.select(doc_ids)

    # 7. Construire la liste de recommandations
    recommendations = [
        {"title": retrieved_movies[i]["title"], "overview": retrieved_movies[i]["text"]}
        for i in range(len(retrieved_movies))
    ]

    return recommendations


# ============================
# TEST D’APPEL
# ============================
recs = recommend_with_rag("I love horror movies with ghosts", top_n=3)
print("\n=== RECOMMANDATIONS ===")
for i, r in enumerate(recs, 1):
    print(f"\n--- Recommandation {i} ---")
    print("Title   :", r["title"])
    print("Overview:", r["overview"])
    
    # Charger le dataset Hugging Face pour l'utiliser dans app.py

# from datasets import load_from_disk
# dataset_hf = load_from_disk("my_custom_dataset")  # Assurez-vous que ce chemin est correct
